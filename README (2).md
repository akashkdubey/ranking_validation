[![PyPI version](https://img.shields.io/pypi/v/rank-validation?label=PyPI)](https://pypi.org/project/rank-validation/)
[![Downloads](https://static.pepy.tech/badge/rank-validation)](https://pepy.tech/project/rank-validation)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

# **rankâ€‘validation**

> **ğŸ“Š Oneâ€‘liner ranking evaluation for search, recommendationâ€¯&â€¯IR**

`rankâ€‘validation` turns a dataframe of **groundâ€‘truth** âœ¨ *vs* **prediction** ğŸ”® into readyâ€‘toâ€‘export **perâ€‘query** and **overall** reports equipped with industryâ€‘standard metrics â€“ all in a single call.

---

## âœ¨ Whatâ€™s newÂ (v1.2)

* **Robust edge handling** â€“Â all metrics now cope with *shorterâ€‘thanâ€‘k* lists and returnÂ 0â€¯instead of crashingÂ îˆ€fileciteîˆ‚turn4file2îˆ  
* **Ï„â€‘ap & RBO fixes** â€“Â no more index errors; two empty lists now yieldÂ RBOâ€¯=â€¯0Â îˆ€fileciteîˆ‚turn4file6îˆ  
* **Cleaner RBO API** â€“â€¯optional `p` lets you tune topâ€‘weighting steepness  
* **Extensible registry** â€“ register a metric with one line:  
  ```python
  METRIC_REGISTRY["my_metric"] = my_func
  ```Â îˆ€fileciteîˆ‚turn4file17îˆ  

---

## ğŸ”‘ Key features

- **Simple pandas API** â€“ `get_metrics_report()` returns familiar `DataFrame`s.  
- **Edgeâ€‘safe implementations** â€“ never raises on zeroâ€‘division or short lists.  
- **Outâ€‘ofâ€‘theâ€‘box metrics** â€“ nDCG, Recall, *Kendallâ€™sâ€¯Ï„â€‘b*, *Kendallâ€™sâ€¯Ï„â€‘ap*, RBO.  
- **Arbitrary cutâ€‘offs** â€“ evaluate atâ€¯@1,â€¯@5,â€¯@20â€¦ whatever matters.  
- **Vectorised NumPyÂ &Â Pandas core** â€“ millions of queries fit on a laptop.  
- **Pure Pythonâ€¯â‰¥â€¯3.8** â€“ zero native extensions.

---

## ğŸš€ Installation

```bash
pip install rank-validation
```

The wheel is tiny (<â€¯30â€¯KB) and depends only on **numpy**, **pandas**, **scipy** & **rbo**.

---

## âš¡ Quick start

```python
import pandas as pd
from rank_validation.validation_generator import get_metrics_report

df = pd.DataFrame({
    "query": ["q1", "q2"],
    "truth_items":  [["A","B","C","D"], ["X","Y","Z"]],
    "truth_scores": [[3,2,1,0],          [2,1,0]],
    "pred_items":   [["B","A","E","C"],  ["Y","X","Z"]],
})

metrics  = ["ndcg", "recall", "kendall_tau", "tau_ap", "rbo"]
cutoffs  = [3, 5]

query_report, overall_report = get_metrics_report(
    df,
    truth_item_col="truth_items",
    truth_score_col="truth_scores",
    pred_item_col="pred_items",
    metric_list=metrics,
    cutoff_list=cutoffs,
)

print(query_report.head())   # perâ€‘query breakdown
print(overall_report.loc["mean"])  # summary row
```

---

## ğŸ§® Supported metrics

| Metric | Measures | Notes |
| ------ | -------- | ----- |
| **nDCG@k** | Graded relevance with logâ‚‚ discount | JÃ¤rvelinâ€¯&â€¯KekÃ¤lÃ¤inenâ€¯2002 |
| **Recall@k** | Share of relevant items retrieved | â€“ |
| **Kendallâ€¯Ï„â€‘b@k** | Rank correlation (tieâ€‘aware) | Kendallâ€¯1938 |
| **Kendallâ€¯Ï„â€‘ap@k** | **Topâ€‘weighted** rank correlation | YÄ±lmazâ€¯etâ€¯al.â€¯2008 |
| **RBO@k (pÂ =Â 0.9)** | Topâ€‘weighted list overlap | Webberâ€¯etâ€¯al.â€¯2010 |

> **Note**Â RBO now returnsÂ 0.0 when both lists are empty and allows custom `p`.

---

## ğŸ› ï¸ Minimal API

```python
def get_metrics_report(
    df: pd.DataFrame,
    truth_item_col: str,
    truth_score_col: str,
    pred_item_col: str,
    metric_list: list[str],
    cutoff_list: list[int],
) -> tuple[pd.DataFrame, pd.DataFrame]
```

*Returns `(query_report, overall_report)` where*  
â€¢ **query_report** â€“ original rows + metric columns  
â€¢ **overall_report** â€“ `query_report.describe()` (use the *mean* row for systemâ€‘level scores)

---

## âš™ï¸ Performance tips

- The heavy lifting is vectorised; still, for **very** deep lists (>â€¯1â€¯K) slice `cutoff_list` to what you care about.  
- Evaluation is embarrassingly parallel; chunk `df` and `concat` if working with 10â€‘millionâ€‘query logs.

---

## ğŸ¤ Contributing

Bug, idea or metric missing? Open an issue or PR! Please add unit tests â€“ metrics are missionâ€‘critical.

---

## ğŸ›£ï¸ Roadmap

- [ ] **MAP** â€“ MeanÂ AverageÂ Precision  
- [ ] **MRR** â€“ MeanÂ ReciprocalÂ Rank  
- [ ] **Precision@k** & **F1@k**  
- [ ] **ERR** â€“ ExpectedÂ ReciprocalÂ Rank  
- [ ] Optional multiâ€‘process evaluation  
- [ ] GPU acceleration via cuDF / RAPIDS  

---

## ğŸ“ License

MIT Â©â€¯2025â€¯Akashâ€¯Dubey

---

## ğŸ”— LinksÂ &Â citation

* Docs / examples: <https://github.com/akashkdubey/ranking_validation>  
* PyPI: <https://pypi.org/project/rank-validation/>

```bibtex
@software{Dubey_2025_rank_validation,
  author = {Dubey, Akash},
  title  = {rankâ€‘validation: A lightweight toolkit for ranking evaluation},
  year   = {2025},
  url    = {https://github.com/akashkdubey/ranking_validation}
}
```

<sub>Built with â¤ï¸Â Pandas, NumPy &Â SciPy.</sub>
