[![PyPI version](https://badge.fury.io/py/rank-validation.svg)](https://badge.fury.io/py/rank-validation) 
[![Downloads](https://static.pepy.tech/badge/rank-validation)](https://pepy.tech/project/rank-validation)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

# **rankâ€‘validation**

> **ğŸ“Š Oneâ€‘liner ranking evaluation for search, recommendation & IR.**

`rankâ€‘validation` turns a dataframe of truth âœ¨ *vs* prediction ğŸ”® into two readyâ€‘toâ€‘export reportsâ€”perâ€‘query and overallâ€”complete with industryâ€‘standard metrics at any cutâ€‘off **k**.

---

## âœ¨ Key features

- **Simple API** â€“ `get_metrics_report(...)` returns pandas DataFrames you already know how to use.  
- **Outâ€‘ofâ€‘theâ€‘box metrics** â€“ nDCG, Recall, Kendallâ€™sÂ Ï„, RBOÂ (extendable).  
- **Arbitrary cutâ€‘offs** â€“ evaluate atÂ @1,Â @5,Â @20â€¦ whatever matters.  
- **Automatic score alignment** â€“ helper utilities map prediction lists onto truth scores for graded relevance.  
- **Vectorised NumPyâ€¯&Â Pandas core** â€“ scales to millions of queries on a laptop.  
- **Pure Pythonâ€¯â‰¥â€¯3.8** â€“ zero native extensions.

---

## ğŸš€ Installation

```bash
pip install rank-validation
```

The wheel is lightweight (<â€¯30â€¯KB) and pulls in only **numpy**, **pandas**, **scipy** & **rbo**.

---

## âš¡ Quick start

```python
import pandas as pd
from rank_validation.validation_generator import get_metrics_report

df = pd.DataFrame({
    "query": ["q1", "q2"],
    "truth_items":  [["A","B","C","D"], ["X","Y","Z"]],
    "truth_scores": [[3,2,1,0],          [2,1,0]],
    "pred_items":   [["B","A","E","C"], ["Y","X","Z"]],
})

metrics  = ["ndcg", "recall", "kendall_tau", "rbo"]
cutoffs  = [3, 5]

query_report, overall_report = get_metrics_report(
    df,
    truth_item_col="truth_items",
    truth_score_col="truth_scores",
    pred_item_col="pred_items",
    metric_list=metrics,
    cutoff_list=cutoffs,
)

print(query_report.head())  # perâ€‘query breakdown
print(overall_report)       # summary stats (mean, std, â€¦)
```

Typical output:

```
  query  ndcg@3  recall@3  kendall_tau@3  rbo@3  ndcg@5  recall@5  kendall_tau@5  rbo@5
0    q1    0.91      0.67           0.33   0.79    0.90      1.00           0.33   0.79
1    q2    1.00      0.67           0.67   1.00    1.00      1.00           0.67   1.00
```

---

## ğŸ§® Supported metrics & formulas

| Metric | What it measures | Reference |
| ------ | ---------------- | --------- |
| **nDCG@k** | Graded relevance with logâ€‘discounted gain, normalised by ideal ranking | JÃ¤rvelinâ€¯&â€¯KekÃ¤lÃ¤inen (2002) |
| **Recall@k** | Proportion of groundâ€‘truth items retrieved in topâ€¯k | â€“ |
| **Kendallâ€™sÂ Ï„@k** | Rank correlation, ties handled via normalisation | Kendall (1938) |
| **RBO@k** | Topâ€‘weighted similarity between two indefinite rankings | Webberâ€¯etâ€¯al. (2010) |

> **Headsâ€‘up:** RBO requires the two lists to have unique items and equalised lengths. If you hit `RankingSimilarity` errors, drop duplicates beforehand or omit RBO for that experiment.

---

## ğŸ› ï¸ API reference

```python
def get_metrics_report(
    df: pd.DataFrame,
    truth_item_col: str,
    truth_score_col: str,
    pred_item_col: str,
    metric_list: list[str],
    cutoff_list: list[int],
) -> tuple[pd.DataFrame, pd.DataFrame]
```

| Parameter | Description |
|-----------|-------------|
| **df** | DataFrame containing at minimum the three listâ€‘columns below. |
| **truth_item_col** | Column name containing groundâ€‘truth item IDs. |
| **truth_score_col** | Column name containing relevance grades (same order & length as `truth_item_col`). |
| **pred_item_col** | Column name with systemâ€‘predicted ranked lists. |
| **metric_list** | Any subset of `["ndcg", "recall", "kendall_tau", "rbo"]`. |
| **cutoff_list** | Integers e.g. `[1, 3, 10]`. Each generates `metric@k` columns. |

Returns `(query_report, overall_report)` where:

- **query_report** â€“ original df plus metric columns.  
- **overall_report** â€“ `query_report.describe()` (mean, std, min, maxâ€¦).

---

## âš™ï¸ Performance tips

- Core logic is vectorised; multiâ€‘process pandas handles millions of rows outâ€‘ofâ€‘theâ€‘box.  
- Chunk evaluation if truth lists are extremely long (>â€¯1â€¯K items) to limit memory.

---

## ğŸ¤ Contributing

Found a bug? Want MAP or MRR support? PRs are welcome! Please open an issue first so we can discuss the approach.

1. Fork â¡ï¸ branch â¡ï¸ commit (with tests!)  
2. `preâ€‘commit run -a`  
3. Open a pull request describing the change.

---

## ğŸ›£ï¸ Roadmap

- [ ] Mean Average Precision (MAP)  
- [ ] Mean Reciprocal Rank (MRR)  
- [ ] Optional GPU acceleration via cuDF / RAPIDS  

---

## ğŸ“ License

MIT Â©â€¯2025â€¯AkashÂ Dubey

---

## ğŸ”— Links & citation

- **Docs / examples**: <https://github.com/akashkdubey/ranking_validation>  
- **PyPI**: <https://pypi.org/project/rank-validation/>

```bibtex
@software{Dubey_2025_rank_validation,
  author = {Dubey, Akash},
  title  = {rankâ€‘validation: A lightweight toolkit for ranking evaluation},
  year   = {2025},
  url    = {https://github.com/akashkdubey/ranking_validation}
}
```

<sub>Built with â¤ï¸, PandasÂ &Â SciPy.</sub>
