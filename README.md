[![PyPI version](https://img.shields.io/pypi/v/rank-validation?label=PyPI)](https://pypi.org/project/rank-validation/)
[![Downloads](https://static.pepy.tech/badge/rank-validation)](https://pepy.tech/project/rank-validation)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)




# **rankâ€‘validation**

> **ğŸ“Š Oneâ€‘liner ranking evaluation for search, recommendation & IR.**

`rankâ€‘validation` turns a dataframe of truth âœ¨ *vs* prediction ğŸ”® into two readyâ€‘toâ€‘export reportsâ€”perâ€‘query and overallâ€”complete with industryâ€‘standard metrics at any cutâ€‘off **k**.

---

## âœ¨ Key features

- **Simple API** â€“ `get_metrics_report(...)` returns pandas DataFrames you already know how to use.  
- **Outâ€‘ofâ€‘theâ€‘box metrics** â€“ nDCG, Recall, Kendallâ€™sâ€¯Ï„â€‘b, Kendallâ€™sâ€¯Ï„â€‘ap, RBO (extendable).  
- **Arbitrary cutâ€‘offs** â€“ evaluate atâ€¯@1,â€¯@5,â€¯@20â€¦ whatever matters.  
- **Automatic score alignment** â€“ helper utilities map prediction lists onto truth scores for graded relevance.  
- **Vectorised NumPyâ€¯&â€¯Pandas core** â€“ scales to millions of queries on a laptop.  
- **Pure Pythonâ€¯â‰¥â€¯3.8** â€“ zero native extensions.

---

## ğŸ“¢Â Whatâ€™s newÂ *(v1.2.0)*

* **Edgeâ€‘safe metrics**
  * nDCG copes with queries shorter than *k*.
  * Ï„â€‘ap avoids outâ€‘ofâ€‘range indexing for tiny lists.
  * RBO returnsÂ `0.0` (notÂ `1.0`) for two empty lists.
* **Robust helpers** &mdash; Utility functions align truth/prediction lists and zeroâ€‘pad scores.
* **Better docs** &mdash; Input schema, edgeâ€‘case semantics and result interpretation are now documented.

---

## ğŸš€ Installation

```bash
pip install rank-validation
```

The wheel is lightweight (<â€¯30â€¯KB) and pulls in only **numpy**, **pandas**, **scipy** & **rbo**.

---

## âš¡ Quick start

```python
import pandas as pd
from rank_validation.validation_generator import get_metrics_report

df = pd.DataFrame({
    "query": ["q1", "q2"],
    "truth_items":  [["A","B","C","D"], ["X","Y","Z"]],
    "truth_scores": [[3,2,1,0],          [2,1,0]],
    "pred_items":   [["B","A","E","C"], ["Y","X","Z"]],
})

metrics  = ["ndcg", "recall", "kendall_tau", "tau_ap", "rbo"]
cutoffs  = [3, 5]

query_report, overall_report = get_metrics_report(
    df,
    truth_item_col="truth_items",
    truth_score_col="truth_scores",
    pred_item_col="pred_items",
    metric_list=metrics,
    cutoff_list=cutoffs,
)

print(query_report.head())  # perâ€‘query breakdown
print(overall_report)       # summary stats (mean, std, â€¦)
```

Typical `query_report`:

```
  query  ndcg@3  recall@3  kendall_tau@3  tau_ap@3  rbo@3  ndcg@5  recall@5  kendall_tau@5  tau_ap@5  rbo@5
0    q1    0.91      0.67           0.33      0.40   0.79    0.90      1.00           0.33      0.46   0.79
1    q2    1.00      0.67           0.67      0.80   1.00    1.00      1.00           0.67      0.80   1.00
```

Typical `overall_report`:

```
       ndcg@3  recall@3  kendall_tau@3  tau_ap@3  rbo@3  ndcg@5  recall@5  kendall_tau@5  tau_ap@5  rbo@5
mean     0.96     0.67           0.50      0.60   0.90    0.95     1.00           0.50      0.63   0.90
std      0.06     0.00           0.24      0.28   0.15    0.05     0.00           0.24      0.24   0.15
```

---

## ğŸ§® Supported metrics & formulas

| Metric | What it measures | Reference |
| ------ | ---------------- | --------- |
| **nDCG@k** | Graded relevance with logâ€‘discounted gain, normalised by ideal ranking | JÃ¤rvelinâ€¯&â€¯KekÃ¤lÃ¤inenâ€¯(2002) |
| **Recall@k** | Proportion of groundâ€‘truth items retrieved in topâ€¯k | â€“ |
| **Kendallâ€™sâ€¯Ï„â€‘b@k** | Rank correlation, tieâ€‘adjusted | Kendallâ€¯(1938) |
| **Kendallâ€™sâ€¯Ï„â€‘ap@k** | **Topâ€‘weighted** rank correlation | Yilmazâ€¯etâ€¯al.â€¯(2008) |
| **RBO@k** | Topâ€‘weighted similarity between two indefinite rankings | Webberâ€¯etâ€¯al.â€¯(2010) |

> **Headsâ€‘up:** RBO requires the two lists to have unique items and equalised lengths. If you hit `RankingSimilarity` errors, drop duplicates beforehand or omit RBO for that experiment.

---

## ğŸ› ï¸ API reference

```python
def get_metrics_report(
    df: pd.DataFrame,
    truth_item_col: str,
    truth_score_col: str,
    pred_item_col: str,
    metric_list: list[str],
    cutoff_list: list[int],
) -> tuple[pd.DataFrame, pd.DataFrame]
```

| Parameter | Description |
|-----------|-------------|
| **df** | DataFrame with at least the three listâ€‘columns below. |
| **truth_item_col** | Column holding groundâ€‘truth item IDs. |
| **truth_score_col** | Column with relevance grades (same order & length). |
| **pred_item_col** | Column holding systemâ€‘predicted ranked lists. |
| **metric_list** | Any subset of `METRIC_REGISTRY` keys, e.g. `ndcg`, `tau_ap`. |
| **cutoff_list** | Integers e.g. `[1, 3, 10]`. Each yields `metric@k` columns. |

Returns `(query_report, overall_report)` where:

* **query_report** â€“ original df plus metric columns.  
* **overall_report** â€“ `query_report.describe()`.

---

### Edgeâ€‘case semantics

* **Empty `truth_items`** â†’ all metricsÂ `0.0` for that query.  
* **Empty `pred_items`** â†’ recallÂ `0.0`; correlation/similarity metrics alsoÂ `0.0`.  
* **Lists shorter thanÂ *k*** â†’ missing ranks are treated as zero gain/irrelevant.

---

## âš™ï¸ Performance tips

- Core logic is vectorised; multiâ€‘process pandas handles millions of rows outâ€‘ofâ€‘theâ€‘box.  
- Chunk evaluation if truth lists are extremely long (>â€¯1â€¯K items) to limit memory.

---

## ğŸ¤ Contributing

Bug report? New metric? Glad to have you! Please:

1. Open an issue outlining the proposal.  
2. Fork â†’ branch â†’ **add unit tests**.  
3. Run `preâ€‘commit run -a` & `pytest`.  
4. Submit a pull request.

---

## ğŸ›£ï¸ Roadmap

- [ ] Mean Average Precision (MAP)  
- [ ] Mean Reciprocal Rank (MRR)  
- [ ] Precision@k & F1@k  
- [ ] Expected Reciprocal Rank (ERR)  
- [ ] GPU acceleration via cuDF / RAPIDS  

---

## ğŸ“ License

ApacheÂ LicenseÂ 2.0 Â©Â 2025Â AkashÂ Dubey

---

## ğŸ”— Links & citation

- **Docs / examples**: <https://github.com/akashkdubey/ranking_validation>  
- **PyPI**: <https://pypi.org/project/rank-validation/>

```bibtex
@software{Dubey_2025_rank_validation,
  author = {Dubey, Akash},
  title  = {rankâ€‘validation: A lightweight toolkit for ranking evaluation},
  year   = {2025},
  url    = {https://github.com/akashkdubey/ranking_validation}
}
```

<sub>Built with â¤ï¸, Pandasâ€¯&â€¯SciPy.</sub>
